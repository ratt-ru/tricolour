#!/usr/bin/env python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import contextlib
from functools import wraps
import logging
from multiprocessing.pool import ThreadPool, cpu_count

import dask
import dask.array as da
from dask.diagnostics import (ProgressBar, Profiler,
                                  ResourceProfiler,
                                  CacheProfiler, visualize)

import numpy as np
import xarray as xr

from xarrayms import xds_from_ms, xds_from_table, xds_to_table

try:
    import bokeh
    can_profile = True
except ImportError:
    can_profile = False


from tricolour.util import aggregate_chunks
from tricolour.config import collect
from tricolour.dask import (sum_threshold_flagger,
                            polarised_intensity,
                            unpolarised_intensity,
                            check_baseline_ordering)
from tricolour.stokes import stokes_corr_map



log = logging.getLogger("tricolour")


def load_config(config_file):
    """
    Parameters
    ----------
    config_file : str

    Returns
    -------
    str
      Configuration file name
    dict
      Configuration
    """
    if config_file == "":
      return config_file, {}

    config = collect([config_file])

    # Load configuration from file if present
    try:
        flagger_kwargs = config['sum_threshold']
    except KeyError:
        log.warn("Configuration file '%s' did not "
                 "contain a 'sum_threshold' key", filename)
        flagger_kwargs = {}

    return config_file, flagger_kwargs


def create_parser():
    formatter = argparse.ArgumentDefaultsHelpFormatter
    p = argparse.ArgumentParser(formatter_class=formatter)
    p.add_argument("ms", help="Measurement Set")
    p.add_argument("-c", "--config", default="",
                   required=False, type=load_config,
                   help="YAML config file containing parameters for "
                   "the flagger in the 'sum_threshold' key.")
    p.add_argument("-if", "--ignore-flags", action="store_true")
    p.add_argument("-fs", "--flagging-strategy", default="standard",
                   choices=["standard", "polarisation"],
                   help="Flagging Strategy. "
                        "If 'standard' all correlations in the visibility "
                          "are flagged independently. "
                        "If 'polarisation' the polarised intensity "
                          "sqrt(Q^2 + U^2 + V^2) is "
                          "calculated and used to flag all correlations "
                          "in the visibility.")
    p.add_argument("-rc", "--row-chunks", type=int, default=10000,
                    help="Hint indicating the number of Measurement Set rows "
                         "to read in a single chunk. "
                         "Smaller and larger numbers will tend to "
                         "respectively decrease or increase both memory usage "
                         "and computational efficiency")
    p.add_argument("-nw", "--nworkers", type=int, default=cpu_count()*2,
                    help="Number of workers (threads) to use. "
                         "By default, set to twice the "
                         "number of logical CPUs on the system. "
                         "Many workers can also affect memory usage "
                         "on systems with many cores.")

    return p


if __name__ == "__main__":
    args = create_parser().parse_args()
    cfg_file, flagger_kwargs = args.config

    # Group datasets by these columns
    group_cols = ["FIELD_ID", "DATA_DESC_ID", "SCAN_NUMBER"]
    # Index datasets by these columns
    index_cols = ['TIME']

    xds = list(xds_from_ms(args.ms,
                           columns=("TIME", "ANTENNA1", "ANTENNA2"),
                           group_cols=group_cols,
                           index_cols=index_cols,
                           chunks={"row": 1e9}))

    # Find the unique times and their row counts
    utime_counts = [da.unique(ds.TIME.data, return_counts=True)
                    for i, ds in enumerate(xds)]
    utime_counts = dask.compute(utime_counts)[0]
    scan_rows = tuple(counts for _, counts in utime_counts)
    scan_chunks = [da.from_array(rc, chunks=ut.size)
                         for ut, rc in utime_counts]
    assert len(scan_rows) == len(xds)

    # Ensure that baseline ordering is consistent per timestep
    dask.compute([check_baseline_ordering(ds.ANTENNA1.data,
                                          ds.ANTENNA2.data,
                                          chunks,
                                          g=g)
                  for g, (ds, chunks)
                  in enumerate(zip(xds, scan_chunks))])

    # Determine how many rows we should handle at once.
    # The row chunk sizes in scan rows already correspond to
    # single timesteps, so choose the maximum supplied either by the user
    # or discovered intrinsically in the data itself
    row_chunks = max(args.row_chunks, max(c.max() for c in scan_rows))

    # Aggregate time and row chunks into the number of timesteps per scan
    agg_time, agg_row = zip(*[aggregate_chunks(((1,) * len(counts), counts),
                                               (len(counts), row_chunks))
                              for utime, counts in utime_counts])

    # Reopen the datasets using the aggregated row ordering
    xds = list(xds_from_ms(args.ms,
                           columns=("DATA", "FLAG"),
                           group_cols=group_cols,
                           index_cols=index_cols,
                           chunks=[{"row": r} for r in agg_row]))

    # Get datasets for DATA_DESCRIPTION and POLARIZATION,
    # partitioned by row
    data_desc_tab = "::".join((args.ms,"DATA_DESCRIPTION"))
    ddid_ds = list(xds_from_table(data_desc_tab, group_cols="__row__"))
    pol_tab = "::".join((args.ms,"POLARIZATION"))
    pds = list(xds_from_table(pol_tab, group_cols="__row__"))

    # Add data from the POLARIZATION table into the dataset
    def _add_pol_data(ds):
        ddid = ddid_ds[ds.attrs['DATA_DESC_ID']].drop('table_row')
        pol = pds[ddid.POLARIZATION_ID.values].drop('table_row')
        return ds.assign(CORR_TYPE=pol.CORR_TYPE,
                         CORR_PRODUCT=pol.CORR_PRODUCT)

    xds = [_add_pol_data(ds) for ds in xds]

    write_computes = []

    # Iterate through each dataset
    for ds, agg_time_counts, row_counts in zip(xds, agg_time, scan_rows):
        row_counts = np.asarray(row_counts)
        ntime, nbl = row_counts.size, row_counts[0]
        nrow, nchan, ncorr = ds.DATA.data.shape
        chunks = da.from_array(row_counts, chunks=(agg_time_counts,))

        # Visibilities from the dataset
        vis = ds.DATA.data

        # Generate unflagged defaults if we should ignore existing flags
        # otherwise take flags from the dataset
        if args.ignore_flags == True:
            flags = da.full_like(vis, False, dtype=np.bool)
        else:
            flags = ds.FLAG.data

        # Reorder vis and flags into katdal-like format
        # (ntime, nchan, ncorrprod). Chunk the corrprod
        # dimension into groups of 64 baselines
        vis = vis.reshape(ntime, nbl, nchan, ncorr)
        flags = flags.reshape(ntime, nbl, nchan, ncorr)

        # Rechunk on baseline dimension
        vis = vis.rechunk({1: 64})
        flags = flags.rechunk({1: 64})

        # If we're flagging on polarised intensity,
        # we convert visibilities to polarised intensity
        # and any flagged correlation will flag the entire visibility
        if args.flagging_strategy == "polarisation":
            corr_type = ds.CORR_TYPE.data.compute().tolist()
            stokes_map = stokes_corr_map(corr_type)
            stokes_pol = tuple(v for k, v in stokes_map.items() if k != 'I')

            vis = polarised_intensity(vis, stokes_pol)
            flags = da.any(flags, axis=3, keepdims=True)
            xncorr = 1
        elif args.flagging_strategy == "standard":
            xncorr = ncorr
        else:
            raise ValueError("Invalid flagging Strategy %s" %
                              args.flagging_strategy)

        vis = vis.transpose(0, 2, 1, 3)
        vis = vis.reshape((ntime, nchan, nbl*xncorr))
        flags = flags.transpose(0, 2, 1, 3)
        flags = flags.reshape((ntime, nchan, nbl*xncorr))

        # Run the flagger
        new_flags = sum_threshold_flagger(vis, flags, chunks, **flagger_kwargs)

        # Reorder flags from katdal-like format back to the MS ordering
        # (ntime*nbl, nchan, ncorr)
        new_flags = new_flags.reshape((ntime, nchan, nbl, xncorr))
        new_flags = new_flags.transpose(0, 2, 1, 3)
        new_flags = new_flags.reshape((-1, nchan, xncorr))

        # Polarised flagging, broadcast the single correlation
        # back to the full correlation range (all flagged)
        if args.flagging_strategy == "polarisation":
            new_flags = da.broadcast_to(new_flags, (ntime*nbl, nchan, ncorr))

        # Make a single chunk for the write back to disk
        new_flags = new_flags.rechunk(new_flags.shape)
        new_ms = ds.assign(FLAG=xr.DataArray(new_flags, dims=ds.FLAG.dims))

        writes = xds_to_table(new_ms, args.ms, "FLAG")
        write_computes.append(writes)

profilers = ([Profiler(), CacheProfiler(), ResourceProfiler()]
              if can_profile else [])
contexts = [ProgressBar()] + profilers


pool = ThreadPool(args.nworkers)

with contextlib.nested(*contexts), dask.config.set(pool=pool):
        dask.compute(write_computes)

if can_profile:
    visualize(profilers)
