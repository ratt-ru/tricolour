#!/usr/bin/env python
from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import argparse
import logging
from functools import wraps

import dask
import dask.array as da
from dask.diagnostics import ProgressBar
import numpy as np
import xarray as xr

from xarrayms import xds_from_ms, xds_to_table

from tricolour import sum_threshold_flagger
from tricolour.util import aggregate_chunks
from tricolour.config import collect


log = logging.getLogger("tricolour")


def load_config(config_file):
    return config_file, collect([config_file])


def create_parser():
    p = argparse.ArgumentParser()
    p.add_argument("ms", help="Measurement Set")
    p.add_argument("-c", "--config", required=False, type=load_config,
                   help="YAML config file containing parameters for "
                   "the flagger in the 'sum_threshold' key.")
    return p


if __name__ == "__main__":

    args = create_parser().parse_args()

    # Group datasets by these columns
    group_cols = ["FIELD_ID", "DATA_DESC_ID", "SCAN_NUMBER"]
    # Index datasets by these columns
    index_cols = ['TIME']

    xms = list(xds_from_ms(args.ms,
                           columns=("TIME", "ANTENNA1", "ANTENNA2"),
                           group_cols=group_cols,
                           index_cols=index_cols,
                           chunks={"row": 1e9}))

    # Find the unique times and their row counts
    utime_chunks = [da.unique(ms.TIME.data, return_counts=True)
                    for i, ms in enumerate(xms)]
    utime_chunks = dask.compute(utime_chunks)[0]
    assert all(len(t) == 2 for t in utime_chunks)
    # Set up dask chunks for the unique timesteps and their row counts
    time_row_chunks = [((1,) * ut.size, rc) for ut, rc in utime_chunks]
    utime, rows = zip(*time_row_chunks)
    # Aggregate time and row chunks into the number of timesteps per scan
    agg_time, agg_row = zip(*[aggregate_chunks((tc, rc), (len(tc), 1e9))
                              for tc, rc in time_row_chunks])

    # Ensure that baseline ordering is consistent per timestep
    for g, (ms, atc, rc) in enumerate(zip(xms, agg_time, rows)):
        ant1, ant2 = da.compute(ms.ANTENNA1.data, ms.ANTENNA2.data)
        start = 0

        for c, chunk in enumerate(rc):
            end = start + chunk
            ant1_ok = np.all(ant1[start:end] == ant1[0:rc[0]])
            ant2_ok = np.all(ant2[start:end] == ant2[0:rc[0]])

            if not ant1_ok or not ant2_ok:
                raise ValueError("Baseline ordering for chunk %d in group %d "
                                 "is inconsistent with other chunks. "
                                 "Fully generally Measurement Sets are not "
                                 "yet supported\n"
                                 "%s != %s\n"
                                 "%s != %s" % (c, g,
                                               ant1[start:end], ant1[0:rc[0]],
                                               ant2[start:end], ant2[0:rc[0]]))

    # Repoen the datasets using the aggregated row ordering
    xms = list(xds_from_ms(args.ms,
                           columns=("TIME", "DATA", "FLAG"),
                           group_cols=group_cols,
                           index_cols=index_cols,
                           chunks=[{"row": r} for r in agg_row]))

    def flagger(vis, flag, chunks, **kwargs):
        token = da.core.tokenize(vis, flag, chunks, kwargs)
        name = '-'.join(('flagger', token))
        dims = ("row", "chan", "corr")

        dsk = da.core.top(sum_threshold_flagger, name, dims,
                          vis.name, dims,
                          flag.name, dims,
                          chunks.name, ("row",),
                          numblocks={
                              vis.name: vis.numblocks,
                              flag.name: flag.numblocks,
                              chunks.name: chunks.numblocks
                          },
                          **kwargs)

        # Add input graphs to the graph
        dsk.update(vis.__dask_graph__())
        dsk.update(flag.__dask_graph__())
        dsk.update(chunks.__dask_graph__())

        return da.Array(dsk, name, vis.chunks, dtype=flag.dtype)

    assert len(agg_time) == len(xms)
    assert len(rows) == len(xms)

    # Load configuration from file if present
    if args.config:
        filename, config = args.config
        try:
            flagger_kwargs = config['sum_threshold']
        except KeyError:
            log.warn("Configuration file '%s' did not "
                     "contain a 'sum_threshold' key", filename)
            flagger_kwargs = {}
    else:
        flagger_kwargs = {}

    write_computes = []

    for ms, atc, rc in zip(xms, agg_time, rows):
        if np.unique(rc).size != 1:
            raise ValueError("Inconsistent number of baselines per timestep. "
                             "All baselines must be present.")

        ntime, nbl = atc[0], rc[0]
        nrow, nchan, ncorr = ms.DATA.data.shape

        # Reorder vis and flags into katdal-like format
        # (ntime, nchan, ncorrprod). Chunk the corrprod
        # dimension into groups of 128 correlation products
        vis = ms.DATA.data.reshape(ntime, nbl, nchan, ncorr)
        vis = vis.transpose(0, 2, 1, 3).reshape((ntime, nchan, -1))
        vis = vis.rechunk({2: 128})

        flags = ms.FLAG.data.reshape(ntime, nbl, nchan, ncorr)
        flags = flags.transpose(0, 2, 1, 3).reshape((ntime, nchan, -1))
        flags = flags.rechunk({2: 128})

        chunks = da.from_array(rc, chunks=(atc,))
        new_flags = flagger(vis, flags, chunks, **flagger_kwargs)

        # Reorder flags from katdal-like format back to the MS ordering
        # (ntime*nbl, nchan, ncorr)
        new_flags = new_flags.reshape((ntime, nchan, nbl, ncorr))
        new_flags = new_flags.transpose(0, 2, 1, 3)
        new_flags = new_flags.reshape((-1, nchan, ncorr))

        new_ms = ms.assign(FLAG=xr.DataArray(new_flags, dims=ms.FLAG.dims))

        writes = xds_to_table(new_ms, args.ms, "FLAG")
        write_computes.append(writes)

with ProgressBar():
    dask.compute(write_computes)
